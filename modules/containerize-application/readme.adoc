= Digital Modernization

:imagesdir: ../../images
:icons: font

== Modern Container Application on EKS

****
*Expected Outcome:*

* Build the Pet Store application within a Container.
* Deploy the Pet Store application to a Wildfly based Application Server.

*Lab Requirements:*
Cloud9 IDE

*Average Lab Time:*
30-45 minutes
****

=== Introduction
The Pet Store application is a Java 7 based application that runs on top of the Wildfly (JBoss) application server. The application is built using Maven. We will walk through the steps to build the application using a Maven container then deploy our WAR file to the Wildfly container. We will be using multi-stage builds to facilitate creating a minimal docker container.

Then we will push the container to link:https://aws.amazon.com/ecr/[Amazon Elastic Container Registry (ECR)] and finally deploy it to link:http://aws.amazon.com/eks/[mazon Elastic Container Service for Kubernetes (Amazon EKS)].

=== Building the Dockerfile

Step 1:: To get started, switch to the `containerize-application` folder within this repository.
+
[source,shell]
----
cd ~/environment/aws-modernization-workshop-advanced/modules/containerize-application/
----
+
Step 2:: Double-click to open the `Dockerfile` in the Cloud9 editor, using the *Environment* navigation pane, as shows below:
+
image::dockerfile-nav.png[Dockerfile]
+
The first stage of our Dockerfile will be to build the application. We will use the `maven:3.5-jdk-7` container from Docker Hub as our base image. The first stage of our Dockerfile will need to copy the code into the container, install the dependencies, then build the application using Maven.
+
The second stage of our Dockerfile will be to build our Wildfly application server. We will need to copy the WAR file we created in the first stage to the second stage. The WAR file is built to `/usr/src/app/target/applicationPetstore.war` in the first stage. We will instruct docker to copy that WAR file to the `/opt/jboss/wildfly/standalone/deployments/applicationPetstore.war` in the new stage. We will also have docker copy the `standalone.xml` file to the proper location `/opt/jboss/wildfly/standalone/configuration/standalone.xml`. Wildfly will deploy that application on boot.
+
The *jPetstore* application requires the *PostrgeSQL* driver. We install that during image build by declaring a `RUN` statement in the Dockerfile.
+
Step 3:: To build the application we will execute the following Docker commands from the `terminal` in our CLoud9 IDE. Make sure the terminal session current working directory is `~/environment/aws-modernization-workshop-advanced/modules/containerize-application/` working directory, by running the following command in the `terminal`.
+
[source,shell]
----
docker build -t containerize-application_petstore:latest .
----
+
NOTE: The container build should take around 15 minutes complete.
+
Step 4:: As mentioned, the *jPetstore* application requires a databse. We will use the official *PostgreSQL* image available from Docker Hub. The following command will pull the official image.
+
[source,shell]
----
docker pull postgres:9.6
----
+
Step 5:: We can review that we have the all the neccessary container images by runnming the following command:
+
[source,shell]
----
docker images
----
+
Expected Output:
+
[.output]
----
REPOSITORY                          TAG                 IMAGE ID            CREATED             SIZE
containerize-application_petstore   latest              7582d2850ea5        6 minutes ago       804MB
postgres                            9.6                 8d9572468d97        12 days ago         230MB
lambci/lambda                       python3.6           420212d009b3        5 weeks ago         1.03GB
lambci/lambda                       python2.7           7a436931435e        5 weeks ago         901MB
lambci/lambda                       nodejs4.3           c0914066d9a8        5 weeks ago         931MB
lambci/lambda                       nodejs6.10          74b405a65ed4        5 weeks ago         946MB
lambci/lambda                       nodejs8.10          edf1f613772c        5 weeks ago         960MB
jboss/wildfly                       11.0.0.Final        a12aa93a45f7        3 months ago        700MB
maven                               3.5-jdk-7           5f03adaf2bbf        7 months ago        483MB
----

=== Push the aplication containers to ECR
Next, we're going to take our already containerized applications and push them to link:https://aws.amazon.com/ecr/[Amazon Elastic Container Registry (ECR)]. Amazon ECR is a fully-managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. Amazon ECR is integrated with Amazon Elastic Container Service (ECS), simplifying your development to production workflow.

==== Deploy ECR Repositories
We will first deploy our CloudFormation template which configures our ECR Repositories.

****
CloudFormation creates the following resources:

* `PetStorePostgreSQLRepository` - ECR Respository for PostgresSQL containers.
* `PetStoreFrontendRepository` - ECR Repository for the front-end containers.
****
Step 1:: We need to replace the placeholder for user name in the template file, so as to distinguish tour ECR Repository from those used by the other Workshop attendees.
+
[source,shell]
----
sed -i "s/<UserName>/${USER_NAME}/" ~/environment/aws-modernization-workshop-advanced/modules/containerize-application/petstore-ecr-cf-resources.yaml
----
+
Step 2:: Using the Cloud9 IDE `terminal`, deploy the CloudFormation template using the `aws cli` tool. 
+
[source,shell]
----
aws cloudformation create-stack --stack-name "${USER_NAME}-petstore-ecr" \
--template-body file://petstore-ecr-cf-resources.yaml \
--capabilities CAPABILITY_NAMED_IAM
----
+
Step 3:: Wait for the Template to finish deploying.
+
[source,shell]
----
until [[ `aws cloudformation describe-stacks --stack-name "${USER_NAME}-petstore-ecr" --query "Stacks[0].[StackStatus]" --output text` == "CREATE_COMPLETE" ]]; do  echo "The stack is NOT in a state of CREATE_COMPLETE at `date`";   sleep 30; done && echo "The Stack is built at `date` - Please proceed"
----

===== Pushing the Petstore images to Amazon Elastic Container Registry (ECR)
Now that we have created our ECR Repositories, we can push our Petstore images.

Step 1:: Log into your Amazon ECR registry using the helper provided by the AWS CLI.

[source,shell]
----
$(aws ecr get-login --no-include-email)
----

NOTE: Ignore any `WARNNING` messages.

Step 2:: Use the AWS CLI to get information about the two Amazon ECR repositories that were created for you by the CloudFormation template. One repository will be for the Petstore PostgreSQL backend and the other will be for the Petstore web frontend.
+
[source,shell]
----
aws ecr describe-repositories --repository-name ${USER_NAME}-petstore_postgres ${USER_NAME}-petstore_frontend
----
+
Example output:
+
[.output]
....
{
    "repositories": [
        {
            "registryId": "123456789012",
            "repositoryName": "petstore_postgres",
            "repositoryArn": "arn:aws:ecr:us-west-2:123456789012:repository/petstore_postgres",
            "createdAt": 1533757748.0,
            "repositoryUri": "123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres"
        },
        {
            "registryId": "123456789012",
            "repositoryName": "petstore_frontend",
            "repositoryArn": "arn:aws:ecr:us-west-2:123456789012:repository/petstore_frontend",
            "createdAt": 1533757751.0,
            "repositoryUri": "123456789012.dkr.ecr.us-west-2.amazonaws.com/petstore_frontend"
        }
    ]
}
....
+
Step 3:: Tag the local docker images with the locations of the remote ECR repositories we created using our CloudFormation template. 
+
[source,shell]
----
docker tag postgres:9.6 $(aws ecr describe-repositories --repository-name ${USER_NAME}-petstore_postgres --query=repositories[0].repositoryUri --output=text):latest

docker tag containerize-application_petstore:latest $(aws ecr describe-repositories --repository-name ${USER_NAME}-petstore_frontend --query=repositories[0].repositoryUri --output=text):latest
----
+
Step 4:: Once the images have been tagged, push them to the remote repository.
+
[source,shell]
----
docker push $(aws ecr describe-repositories --repository-name ${USER_NAME}-petstore_postgres --query=repositories[0].repositoryUri --output=text):latest

docker push $(aws ecr describe-repositories --repository-name ${USER_NAME}-petstore_frontend --query=repositories[0].repositoryUri --output=text):latest
----
+
You should see the Docker images being pushed with an output similar to this:
+
[.output]
....
The push refers to repository [<REDACTED>.dkr.ecr.us-west-2.amazonaws.com/petstore_postgres]
7856d1f55b98: Pushed
a125032aca95: Pushed
fcfc309521a9: Pushed
4c4e9f97ac56: Pushed
109402c6a817: Pushed
6663c6c0d308: Pushed
ed4da41a79a9: Layer already exists
7c050956ab95: Layer already exists
c6fcee3b341c: Layer already exists
998e6abcfae7: Layer already exists
df9515382700: Layer already exists
0fae9a7d0574: Layer already exists
add4404d0b51: Layer already exists
cdb3f9544e4c: Layer already exists
latest: digest: sha256:ca39b6107978303706aac0f53120879afcd0d4b040ead7f19e8581b81c19ecea size: 3243
....

=== Deploying to EKS
With the images pushed to Amazon ECR we are ready to deploy them to our orchestrator. Next, we will show you how to leverage link:https://aws.amazon.com/eks/[Amazon EKS] to orchestrate our containers into production.

image:eks.png[ECS]

==== Getting Started with Amazon Elastic Container Service for Kubernetes (EKS)

Before we get started, here are some terms you need to understand in order to deploy your application when creating your first Amazon EKS cluster.

[options="header"]
|=======================
| Object | Cluster
| *Cluster* | A group of EC2 instances that are all running `kubelet` which connects into the master control plane.
| *Deployment* | Configuration file that declares how a container should be deployed including how many `replicas` what the `port` mapping should be and how it is `labeled`.
| *Service* | Configuration file that declares the ingress into the container, these can be used to create Load Balancers automatically.
|=======================

===== Amazon EKS and Kubernetes Manifests

To use Amazon EKS or any Kubernetes cluster you must write a manifest or a config file, these config files are used to declaritively document what is needed to deploy an individual application. More information can be found https://kubernetes.io/docs/concepts/workloads/controllers/deployment/[here].

Step 1:: Open the `petstore-eks-manifest.yaml` file by double clicking the filename in the lefthand navigation of the Cloud9 IDE. The file has the following contents:
+
[source,yaml]
----
apiVersion: v1
kind: List
items:
- apiVersion: v1
  kind: Namespace
  metadata:
    name: petstore

- kind: PersistentVolume
  apiVersion: v1
  metadata:
    name: postgres-pv-volume
    namespace: petstore
    labels:
      type: local
  spec:
    storageClassName: gp2
    capacity:
      storage: 20Gi
    accessModes:
      - ReadWriteMany
    hostPath:
      path: "/mnt/data"

- apiVersion: v1
  kind: PersistentVolumeClaim
  metadata:
    name: postgres-pv-claim
    namespace: petstore
  spec:
    storageClassName: gp2
    accessModes:
      - ReadWriteOnce
    resources:
      requests:
        storage: 20Gi

- apiVersion: v1
  kind: Service
  metadata:
    name: postgres
    namespace: petstore
  spec:
    ports:
    - port: 5432
    selector:
      app: postgres
    clusterIP: None

- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: postgres
    namespace: petstore
  spec:
    selector:
      matchLabels:
        app: postgres
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          app: postgres
      spec:
        containers:
        - image: <YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/<UserName>-petstore_postgres:latest
          name: postgres
          env:
          - name: POSTGRES_PASSWORD
            value: password
          - name: POSTGRES_DB
            value: petstore
          - name: POSTGRES_USER
            value: admin
          ports:
          - containerPort: 5432
            name: postgres
          volumeMounts:
          - name: postgres-persistent-storage
            mountPath: /var/lib/postgresql/data
            subPath: petstore
        volumes:
        - name: postgres-persistent-storage
          persistentVolumeClaim:
            claimName: postgres-pv-claim

- apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    namespace: petstore
  spec:
    selector:
      app: frontend
    ports:
    - port: 80
      targetPort: http-server
      name: http
    - port: 9990
      targetPort: wildfly-cord
      name: wildfly-cord
    type: LoadBalancer

- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    name: frontend
    namespace: petstore
    labels:
      app: frontend
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: frontend
    template:
      metadata:
        labels:
          app: frontend
      spec:
        initContainers:
        - name: init-frontend
          image: <YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/<UserName>-petstore_postgres:latest
          command: ['sh', '-c',
                    'until pg_isready -h postgres.petstore.svc -p 5432;
                    do echo waiting for database; sleep 2; done;']
        containers:
        - name: frontend
          image: <YourAccountID>.dkr.ecr.us-west-2.amazonaws.com/<UserName>-petstore_frontend:latest
          resources:
            requests:
              memory: "512m"
              cpu: "512m"
          ports:
          - name: http-server
            containerPort: 8080
          - name: wildfly-cord
            containerPort: 9990
          env:
          - name: DB_URL
            value: "jdbc:postgresql://postgres.petstore.svc:5432/petstore?ApplicationName=applicationPetstore"
          - name: DB_HOST
            value: postgres.petstore.svc
          - name: DB_PORT
            value: "5432"
          - name: DB_NAME
            value: petstore
          - name: DB_USER
            value: admin
          - name: DB_PASS
            value: password

----
+
NOTE: Amazon EKS clusters that were created prior to Kubernetes version 1.11 were not created with any storage classes. Since we are running the version `1.12`, the default `StorageClass` has already been set to link:https://aws.amazon.com/ebs/[Amazon Elastic Block Store (EBS)]. Therefore there is no `StorageClass` definition int he `petstore-eks-manifest.yaml` file.
+
Step 2:: Close the `petstore-eks-manifest.yaml`. Run the following commands in the Cloud9 IDE `terminale` to replace the *<YourAccountID>* as well as *<UserName>* placeholders with your AWS Account ID and unique AWS IAM 
User.
+
[source,shell]
----
ACCOUNT_ID=$(aws sts get-caller-identity --output text --query 'Account')

sed -i "s/<YourAccountID>/${ACCOUNT_ID}/" petstore-eks-manifest.yaml

sed -i "s/<UserName>/${USER_NAME}/" petstore-eks-manifest.yaml
----
+
Step 3:: Apply your manifest by running this command in your Cloud9 IDE `terminal`:
+
[source,shell]
----
kubectl apply -f petstore-eks-manifest.yaml
----
+
Expected Output:
+
[.output]
....
namespace/petstore created
persistentvolume/postgres-pv-volume configured
persistentvolumeclaim/postgres-pv-claim created
service/postgres created
deployment.apps/postgres created
service/frontend created
deployment.apps/frontend created
....
+
As you can see above this manifest created and configured several components in your Kubernetes cluster, we've created a *namespace*, *persistentvolume*, *persistentvolumeclaim*, 2 *services*, and 2 *deployments*.
+
[options="header"]
|=======================
| Primitive | Description
| *Namespace* | Namespaces are meant to be virtual clusters within a larger pysical cluster.
| *PersistentValue* | Persistent Volume (PV) is a piece of storage that has been provisioned by an administrator. _These are cluster wide resources._
| *PersistentVolumeClaim* | Persistent Volume Claim (PVC) is a request for storage by a user.
| *Service* | Service is an abstraction which defines a logical set of Pods and a policy by which to access them.
| *Deployment* | Deployment controller provides declarative updates for Pods and ReplicaSets.
|=======================
+
Step 4:: Now that the scheduler knows that you want to run this application, it will find available *disk*, *cpu* and *memory* and will place the pods on *Worker Nodes*. Let's watch as they get provisioned.
+
[source,shell]
----
kubectl get pods --namespace petstore --watch
----
+
Expected Output:
+
[.output]
....
NAME                        READY     STATUS              RESTARTS   AGE
frontend-869db5db6b-ht4h8   0/1       Init:0/1            0          3m
frontend-869db5db6b-j5nfj   0/1       Init:0/1            0          3m
postgres-678864b7-vs5zj     0/1       ContainerCreating   5          3m
....
+
Step 5:: Once the *STATUS* changes to *Running* for all 3 of your containers we can then load the services and navigate to the exposed application (you will need to `[ctrl + c]` since its watching).
+
[source,shell]
----
kubectl get services --namespace petstore -o wide
----
+
Expected Output:
+
[.output]
....
NAME       TYPE           CLUSTER-IP      EXTERNAL-IP                                                               PORT(S)                                     AGE
frontend   LoadBalancer   10.100.20.251   ac7059d97a51611e88f630213e88d018-2093299179.us-west-2.elb.amazonaws.com   80:30327/TCP,443:32177/TCP,9990:30543/TCP   6m
postgres   ClusterIP      None            <none>                                                                    5432/TCP                                    6m
....
+
Step 5:: Here we can see that we're exposing the *frontend* using an ELB, which is available at the *EXTERNAL-IP* field. Copy and paste this into a new browser tab. You should see the following web application:
+
image:preview.png[Preview]

Now that we have our containers deployed to Amazon EKS we can continue with the workshop and look at how to monitor the *Pet Store* application.